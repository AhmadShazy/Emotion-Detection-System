{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTA Emotion Detection Live System for Google Colab\n",
    "\n",
    "This notebook provides a **unified live interaction system**. Run the cells in order to initialize the models and start the live dashboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required libraries\n",
    "!pip install -q nemo_toolkit[asr] timm torchaudio soundfile opencv-python-headless huggingface_hub tqdm\n",
    "!apt-get install -y -qq ffmpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Infrastructure: Model Registry & Orchestrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import display, Javascript, clear_output\n",
    "from google.colab import output\n",
    "import base64\n",
    "import io\n",
    "import PIL.Image\n",
    "import time\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import timm\n",
    "\n",
    "class LiveAssistantOrchestrator:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing SOTA Models (this may take a minute)...\")\n",
    "        # 1. NeMo ASR\n",
    "        self.asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"stt_en_conformer_transducer_large\").cuda()\n",
    "        \n",
    "        # 2. Emformer SER (Architecture + Emotion Projection)\n",
    "        from torchaudio.models import Emformer\n",
    "        self.ser_model = Emformer(input_dim=80, num_heads=8, ffn_dim=1024, num_layers=12, segment_length=16, right_context_length=4).cuda()\n",
    "        self.ser_head = nn.Linear(80, 4).cuda() # Happy, Angry, Neutral, Sad\n",
    "        \n",
    "        # 3. ViT Facial Emotion\n",
    "        self.face_model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=7).cuda()\n",
    "        \n",
    "        self.emotions = [\"Neutral\", \"Happy\", \"Sad\", \"Angry\", \"Surprised\", \"Fear\", \"Disgust\"]\n",
    "        print(\"All models loaded successfully on GPU!\")\n",
    "\n",
    "    def process_frame(self, frame_b64):\n",
    "        # Decode image\n",
    "        header, encoded = frame_b64.split(\",\", 1)\n",
    "        data = base64.b64decode(encoded)\n",
    "        image = PIL.Image.open(io.BytesIO(data))\n",
    "        frame = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Facial Emotion Inference\n",
    "        img_tensor = torch.from_numpy(cv2.resize(frame, (224, 224))).permute(2,0,1).float().unsqueeze(0).cuda() / 255.0\n",
    "        with torch.no_grad():\n",
    "            preds = self.face_model(img_tensor)\n",
    "            emotion_idx = torch.argmax(preds, dim=1).item()\n",
    "        \n",
    "        return self.emotions[emotion_idx]\n",
    "\n",
    "    def process_audio(self, audio_b64):\n",
    "        # Placeholder for streaming audio processing\n",
    "        # In a real system, we'd slice the buffer here and feed NeMo/Emformer\n",
    "        return \"Transcription sample...\", \"Neutral\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Real-time Dashboard UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_live_system():\n",
    "    orchestrator = LiveAssistantOrchestrator()\n",
    "    \n",
    "    display(HTML('''\n",
    "        <div style=\"padding: 20px; background: #1e1e1e; color: white; border-radius: 10px; font-family: sans-serif;\">\n",
    "            <h2>\ud83e\udd16 Humanoid Live Assistant - SOTA Dashboard</h2>\n",
    "            <p style=\"color: #aaa;\">Please ensure you click <b>\"Allow\"</b> for Camera and Microphone permissions.</p>\n",
    "            <div id=\"status-box\" style=\"font-size: 1.2em; border-left: 5px solid #00ff00; padding-left: 15px; margin-bottom: 20px;\">\n",
    "                Initializing models... (Wait for \"All models loaded\" message)\n",
    "            </div>\n",
    "            <div id=\"video-container\" style=\"display: flex; justify-content: center;\"></div>\n",
    "        </div>\n",
    "    '''))\n",
    "    \n",
    "    def process_all_wrapper(frame_b64):\n",
    "        try:\n",
    "            face_emo = orchestrator.process_frame(frame_b64)\n",
    "            return output.JSON({\n",
    "                \"face_emotion\": face_emo,\n",
    "                \"voice_emotion\": \"Processing...\",\n",
    "                \"transcript\": \"Listening...\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            return output.JSON({\"error\": str(e)})\n",
    "\n",
    "    # Robust JS with Error Handling\n",
    "    js = Javascript('''\n",
    "        async function runLive() {\n",
    "            const statusBox = document.getElementById('status-box');\n",
    "            try {\n",
    "                const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });\n",
    "                const video = document.createElement('video');\n",
    "                video.style.width = '100%';\n",
    "                video.style.maxWidth = '600px';\n",
    "                video.srcObject = stream;\n",
    "                document.getElementById('video-container').appendChild(video);\n",
    "                await video.play();\n",
    "\n",
    "                const canvas = document.createElement('canvas');\n",
    "                canvas.width = video.videoWidth;\n",
    "                canvas.height = video.videoHeight;\n",
    "                const ctx = canvas.getContext('2d');\n",
    "\n",
    "                statusBox.innerText = \"System Live! Analysing...\";\n",
    "\n",
    "                while (true) {\n",
    "                    ctx.drawImage(video, 0, 0);\n",
    "                    const imgData = canvas.toDataURL('image/jpeg', 0.5);\n",
    "                    \n",
    "                    try {\n",
    "                        const result = await google.colab.kernel.invokeFunction('notebook.process_all', [imgData], {});\n",
    "                        if (result && result.data) {\n",
    "                            const data = result.data['application/json'];\n",
    "                            if (data.error) {\n",
    "                                statusBox.innerText = `Error: ${data.error}`;\n",
    "                            } else {\n",
    "                                statusBox.innerHTML = `\n",
    "                                    <b>Face Emotion:</b> ${data.face_emotion}<br>\n",
    "                                    <b>Voice Emotion:</b> ${data.voice_emotion}<br>\n",
    "                                    <b>Transcript:</b> ${data.transcript}\n",
    "                                `;\n",
    "                            }\n",
    "                        }\n",
    "                    } catch (e) {\n",
    "                        console.error(\"Inference Error:\", e);\n",
    "                    }\n",
    "                    \n",
    "                    await new Promise(r => setTimeout(r, 200)); // 5 FPS for stability\n",
    "                }\n",
    "            } catch (err) {\n",
    "                statusBox.innerHTML = `<span style=\"color: #ff4444;\">\u274c Access Denied: ${err.message}. Ensure camera/mic permissions are granted.</span>`;\n",
    "            }\n",
    "        }\n",
    "        runLive();\n",
    "    ''')\n",
    "    \n",
    "    output.register_callback('notebook.process_all', process_all_wrapper)\n",
    "    display(js)\n",
    "\n",
    "# EXECUTE THIS TO START\n",
    "start_live_system()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}