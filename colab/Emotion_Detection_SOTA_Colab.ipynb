{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTA Emotion Detection Live System for Google Colab\n",
    "\n",
    "This notebook provides a **unified live interaction system**. Run the cells in order to initialize the models and start the live dashboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required libraries\n",
    "!pip install -q nemo_toolkit[asr] timm torchaudio soundfile opencv-python-headless huggingface_hub tqdm\n",
    "!apt-get install -y -qq ffmpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Infrastructure: Model Registry & Orchestrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import display, Javascript, clear_output\n",
    "from google.colab import output\n",
    "import base64\n",
    "import io\n",
    "import PIL.Image\n",
    "import time\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import timm\n",
    "\n",
    "class LiveAssistantOrchestrator:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing SOTA Models (this may take a minute)...\")\n",
    "        # 1. NeMo ASR\n",
    "        self.asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"stt_en_conformer_transducer_large\").cuda()\n",
    "        \n",
    "        # 2. Emformer SER (Architecture + Emotion Projection)\n",
    "        from torchaudio.models import Emformer\n",
    "        self.ser_model = Emformer(input_dim=80, num_heads=8, ffn_dim=1024, num_layers=12, segment_length=16, right_context_length=4).cuda()\n",
    "        self.ser_head = nn.Linear(80, 4).cuda() # Happy, Angry, Neutral, Sad\n",
    "        \n",
    "        # 3. ViT Facial Emotion\n",
    "        self.face_model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=7).cuda()\n",
    "        \n",
    "        self.emotions = [\"Neutral\", \"Happy\", \"Sad\", \"Angry\", \"Surprised\", \"Fear\", \"Disgust\"]\n",
    "        print(\"All models loaded successfully on GPU!\")\n",
    "\n",
    "    def process_frame(self, frame_b64):\n",
    "        # Decode image\n",
    "        header, encoded = frame_b64.split(\",\", 1)\n",
    "        data = base64.b64decode(encoded)\n",
    "        image = PIL.Image.open(io.BytesIO(data))\n",
    "        frame = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Facial Emotion Inference\n",
    "        img_tensor = torch.from_numpy(cv2.resize(frame, (224, 224))).permute(2,0,1).float().unsqueeze(0).cuda() / 255.0\n",
    "        with torch.no_grad():\n",
    "            preds = self.face_model(img_tensor)\n",
    "            emotion_idx = torch.argmax(preds, dim=1).item()\n",
    "        \n",
    "        return self.emotions[emotion_idx]\n",
    "\n",
    "    def process_audio(self, audio_b64):\n",
    "        # Placeholder for streaming audio processing\n",
    "        # In a real system, we'd slice the buffer here and feed NeMo/Emformer\n",
    "        return \"Transcription sample...\", \"Neutral\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Real-time Dashboard UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_live_system():\n",
    "    orchestrator = LiveAssistantOrchestrator()\n",
    "    \n",
    "    # JavaScript for Continuous Camera Capture\n",
    "    js = Javascript('''\n",
    "        async function runLive() {\n",
    "            const video = document.createElement('video');\n",
    "            video.style.display = 'block';\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });\n",
    "            document.body.appendChild(video);\n",
    "            video.srcObject = stream;\n",
    "            await video.play();\n",
    "\n",
    "            const canvas = document.createElement('canvas');\n",
    "            canvas.width = video.videoWidth;\n",
    "            canvas.height = video.videoHeight;\n",
    "            const ctx = canvas.getContext('2d');\n",
    "\n",
    "            while (true) {\n",
    "                ctx.drawImage(video, 0, 0);\n",
    "                const imgData = canvas.toDataURL('image/jpeg', 0.8);\n",
    "                \n",
    "                // Send frame to Python for processing\n",
    "                const result = await google.colab.kernel.invokeFunction('notebook.process_all', [imgData], {});\n",
    "                \n",
    "                // Display results overlay\n",
    "                if (result) {\n",
    "                    const data = result.data['application/json'];\n",
    "                    // We can update a separate div here with the status\n",
    "                    document.getElementById('status-box').innerText = \n",
    "                        `Face Emotion: ${data.face_emotion}\\\\nVoice Emotion: ${data.voice_emotion}\\\\nTranscript: ${data.transcript}`;\n",
    "                }\n",
    "                \n",
    "                await new Promise(r => setTimeout(r, 100)); // ~10 FPS\n",
    "            }\n",
    "        }\n",
    "    ''')\n",
    "    \n",
    "    # Create HTML layout for the dashboard\n",
    "    from IPython.display import HTML\n",
    "    display(HTML('''\n",
    "        <div style=\"padding: 20px; background: #1e1e1e; color: white; border-radius: 10px; font-family: sans-serif;\">\n",
    "            <h2>\ud83e\udd16 Humanoid Live Assistant - SOTA Dashboard</h2>\n",
    "            <div id=\"status-box\" style=\"font-size: 1.2em; border-left: 5px solid #00ff00; padding-left: 15px; margin-bottom: 20px;\">\n",
    "                Initializing...\n",
    "            </div>\n",
    "            <div id=\"video-container\"></div>\n",
    "        </div>\n",
    "    '''))\n",
    "    \n",
    "    def process_all_wrapper(frame_b64):\n",
    "        face_emo = orchestrator.process_frame(frame_b64)\n",
    "        # For demo purposes, we simulate the voice/transcript logic\n",
    "        return output.JSON({\n",
    "            \"face_emotion\": face_emo,\n",
    "            \"voice_emotion\": \"Processing...\",\n",
    "            \"transcript\": \"Listening...\"\n",
    "        })\n",
    "\n",
    "    output.register_callback('notebook.process_all', process_all_wrapper)\n",
    "    display(js)\n",
    "    output.eval_js('runLive()')\n",
    "\n",
    "# EXECUTE THIS TO START\n",
    "# start_live_system()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}